---
title: "World Analysis"
author: "Min Woo Sun and David Troxell"
date: "5/21/2022"
output: html_document
#Date edited: 9/15/22
---


Summary
- Read-in data from pre-processing script. Do a few other small pre-processing steps

- Fit RandomForest on static + modifiable features + grouped policies. Any combo of these types of features can be tested

- Fit GBM on static + modifiable features + grouped policies. Any combo of these types of features can be tested

-Partial Dependency plots as shown in paper

```{r}
here::i_am("analysis/code/analysis_world_GBMandRF.Rmd")
library(here)
library(dplyr)
library(ggplot2)
library(glmnet)
library(caret)
library(gbm)
library(reshape2)
library(reshape)
library(pdp)
library(plotly)

source(here::here("analysis/code/helper_functions.R"))

set.seed(100)
```





#### Preprocessing
```{r}
# load data
df = read.csv(here::here("analysis/data/preprocessed","XY_WHO_trust.csv"))
iso = df$iso

# exclude features due to either multicollinearity identified in other scripts, or because of variables' status as just a name/identifier
index.exclude = which(names(df) %in% c("iso",
                                       "Days_Until_All_Vulnerable_Vacc_Elig",
                                       "Ages_15_To_64_Percent",
                                       "V1",
                                       "Sub.region.Name",
                                       "Vaccines_Safe_50Plus",
                                       "Health_Expenditure_Per_Capita",
                                       "Ages_0_To_14_Percent",
                                       "Trust_In_Journalists",
                                       "Percent_Health_Expenditure_Private",
                                       "Avg_Gov_Stringency_Index",
                                       "Percent_Ppl_Poor_Air_Quality",
                                       "Percent_Using_Internet",
                                       "Total_Days_Masks_At_Least_Recommended"
                                       ))

# convert region variables into factor
df$Region.Name <- as.factor(df$Region.Name)
df$Sub.region.Name <- as.factor(df$Sub.region.Name)


#filter for 10M. Discussed in paper
N = 5000000
iso=iso[df$Population>N]
Y = df[df$Population>N, "excess_death"]
df = df[df$Population>N,-index.exclude]

# Convert Y into crude rate from count 
X = df %>% select(-excess_death)
Y = (Y / X$Population) * 100000


###########################################
# Log transform and scale skewed features #
###########################################
X[,'Population'] = log(X[,'Population']+1)
X[,"People_Per_Sq_Km_of_Land"] = log(X[,"People_Per_Sq_Km_of_Land"]+.01)
X[,"GDP_Per_Capita"] = log(X[,"GDP_Per_Capita"]+.01)


###########################################
# Transform and Z-score target variable   #
###########################################
# apply none (0) log (1) or cube root (2) transform then scale target
Y = cube_root(Y)

```


```{r}

# one-hot encode region / sub-region variables
##############
# continents #
##############

#define one-hot encoding function
dummy <- dummyVars(" ~ .", data=X)

#perform one-hot encoding on data frame
X.dummy <- data.frame(predict(dummy, newdata=X))

# drop columns with 0 variance -> just Oceania
X.dummy <- X.dummy %>% select(-Region.Name.Oceania)

X = X.dummy


##############
# sub-region #
##############

# #define one-hot encoding function
# dummy <- dummyVars(" ~ .", data=X)
# 
# #perform one-hot encoding on data frame
# X.dummy <- data.frame(predict(dummy, newdata=X))
# 
# # # drop columns with 0 variance -> just Oceania
# # X.dummy <- X.dummy %>% select(-Region.Name.Oceania)
# 
# index.sub <- grep("Sub", names(X.dummy), fixed=TRUE)
# names.sub.region <- names(X.dummy)[index.sub]
# 
# #apply(X.dummy, MARGIN=2, FUN=var)
# 
# X = X.dummy
```



```{r}
##########################################
#             set indices                #
##########################################

# static features
index.static = which(names(X) %in% c("Population",
                                      "Obese_Adult_Percentage",
                                      "Hospital_Beds_Per_1000",
                                      "Nurses_And_Midwives_Per_1000",
                                      "People_Per_Sq_Km_of_Land" ,
                                      "GDP_Per_Capita",
                                      "Age_65_Older_Percent",
                                      "Trust_In_Neighborhood",
                                      "Trust_In_Govt",
                                      "Confidence_In_Hospitals"
                                     ))

# modifiable features -- exclude grouping, policy 
index.modifiable = which(names(X) %in% c("Percent_One_Dose_As_Of_Nov_1",
                                          "Total_Days_Over_1_Test_Per_Thousand"
                                          ))  

#  modifiable policy features without PCA
index.policy.ungrouped = which(names(X) %in% c("Days_Until_All_Vulnerable_Vacc_Elig",
                                                "Days_Until_Masks_Recommended",
                                                "Days_Until_Masks_Required",
                                                "Days_Until_Workplace_Closures_Except_Key",           
                                                "Days_Until_Testing_Key_Groups",  
                                                 "Total_Days_Masks_Required_Public",
                                                "Total_Days_Masks_At_Least_Recommended",
                                                "Total_Days_Workplace_Closures_Except_Key",
                                                "Total_Days_Workplace_Closures_Recommended",
                                                "Total_Days_Stay_At_Home_Required_Except_Essentials",
                                                "Total_Days_Comprehensive_Contact_Tracing",
                                                "Total_Days_Open_Public_Testing"
                                               ))

# modifiable policy features grouped
index.policy.grouped = which(names(X) %in% c("Govt_Swiftness_Stringency",
                                              "Govt_Persistent_Stringency"))

# index for dummy variable continent
index.region = which(names(X) %in% c("Region.Name.Africa",
                                     "Region.Name.Americas",
                                     "Region.Name.Asia",
                                     "Region.Name.Europe"
                                     ))

#modifiable trust
index.trust = which(names(X) %in% c( "Trust_Covid_Advice_Govt"))
```


```{r}
# get indices for countries of interest
index.USA = which(iso == "USA")
index.CAN = which(iso == "CAN")

index.KOR = which(iso == "KOR")
index.DEU = which(iso == "DEU")
index.MEX = which(iso == "MEX")
index.MMR = which(iso == "MMR")

index.ITA = which(iso == "ITA")
index.FRA = which(iso == "FRA")
index.GBR = which(iso == "GBR")
```

```{r}
# Create Function to display Correlation Matrix
displayCorrMat<-function(numericTrustMetrics){
cormat1 <- round(x = cor(numericTrustMetrics), digits = 1)
cormat1[lower.tri(cormat1)]<- NA
melted_cormat1 <- melt(cormat1,as.is = TRUE)
 p<-ggplot(data = melted_cormat1, aes(X1, X2, fill = value))+
   geom_tile(color = "white")+geom_text(aes(label = value),size = 2)+
   scale_fill_gradient2(low = "blue", high = "red", mid = "white", 
                        midpoint = 0, limit = c(-1,1), space = "Lab", 
                        name="Pearson\nCorrelation") +
   theme_minimal()+ 
   theme(axis.text.x = element_text(angle = 75, vjust = 1, 
                                    size = 8, hjust = 1))+
   coord_fixed()
 print(p)
}
```


######################
#### Random Forest
######################
```{r}


set.seed(100)

# select features for X
# user can select any subset of these options or use index.policy.ungrouped as well
fit.indices = c(index.static,
                index.region,
                index.trust,
                index.modifiable,
                index.policy.grouped)

#incorporate the new features
allData<-cbind(X[,fit.indices],Y)

#5 folds repeated 100 times
#save all predictions made throughout CV process. Each point will be predicted 100 times
control <- trainControl(method='repeatedcv', 
                        number=5, 
                        repeats=100,
                        savePredictions ="all") 

#range of hyperparameter values to try
mtry <- seq(1,20)
tunegrid <- expand.grid(mtry=mtry)

#run cv process
rf<- train(Y~.,data=allData, method='rf',tuneGrid=tunegrid, trControl=control )

#visualize results
#cv plot
plot(rf)
#variable importance
plot(varImp(rf)) 

#obtain all predictions made in cv process
allPreds<-rf$pred
#obtain best hyperparameter value
finalMtry<-rf$bestTune

#find the CV predictions (i.e. preval) for best hyperparameter
finalSubset<-subset(allPreds,mtry==as.numeric(finalMtry[1]))

preds<-finalSubset$pred
actual<-finalSubset$obs

#get boxplot of all CV RMSE
allRMSERf <-c()
index<-1
for(i in 1:100){
  predsTemp<-preds[index:(index+nrow(allData)-1)]
  actualTemp<-actual[index:(index+nrow(allData)-1)]
  preds_untrsf <- predsTemp^3
  actual_untrsf <-  actualTemp^3
  allRMSERf <- c(allRMSERf,compute_rMSE(preds_untrsf, actual_untrsf))
  index<-index+nrow(allData)
}
boxplot(allRMSERf)

#get these predictions on original scale
preds_untrsf <-  preds^3
actual_untrsf <-  actual^3

#look at overall RMSE and correlation btwn predictions and true values
compute_rMSE(preds_untrsf, actual_untrsf)
cor(preds_untrsf, actual_untrsf, method="spearman")

#investigate individual countries

#get predictions where row index was
USpreds<-finalSubset[finalSubset$rowIndex == index.USA,1]
Canadapreds<-finalSubset[finalSubset$rowIndex == index.CAN,1]
Koreapreds <- finalSubset[finalSubset$rowIndex == index.KOR,1] 
Mexicopreds <- finalSubset[finalSubset$rowIndex == index.MEX,1] 
Germanypreds <- finalSubset[finalSubset$rowIndex == index.DEU,1] 
Myanmarpreds <- finalSubset[finalSubset$rowIndex == index.MMR,1] 

#get these predictions on original scale
us_preds_untrsf <-  USpreds^3
can_preds_untrsf <- Canadapreds^3
kor_preds_untrsf <- Koreapreds^3

deu_preds_untrsf <- Germanypreds^3
mex_preds_untrsf <- Mexicopreds^3
mmr_preds_untrsf <- Myanmarpreds^3

#get actual values
us_actual<-Y[index.USA]^3
canada_actual<-Y[index.CAN]^3
korea_actual<-Y[index.KOR]^3
germany_actual<-Y[index.DEU]^3
mexico_actual<-Y[index.MEX]^3
myanmar_actual<-Y[index.MMR]^3


#investigate
paste0("USA observed: ",us_actual)
paste0("USA predicted: ", mean(us_preds_untrsf))

paste0("CAN observed: ",canada_actual)
paste0("CAN predicted: ", mean(can_preds_untrsf))

paste0("KOR observed: ", korea_actual)
paste0("KOR predicted: ", mean(kor_preds_untrsf))

paste0("DEU observed: ", germany_actual)
paste0("DEU predicted: ", mean(deu_preds_untrsf))

paste0("MEX observed: ", mexico_actual)
paste0("MEX predicted: ", mean(mex_preds_untrsf))

paste0("MMR observed: ", myanmar_actual)
paste0("MMR predicted: ", mean(mmr_preds_untrsf))
```

#####################
# Gradient Boosting 
#####################
```{r}

set.seed(100)

# select features for X
# user can select any subset of these options or use index.policy.ungrouped as well
fit.indices = c(index.static,
                index.region,
                index.trust,
                index.modifiable,
                index.policy.grouped)

#incorporate the new features
allData<-cbind(X[,fit.indices],Y)

#5 folds repeated 100 times
#save all predictions made throughout CV process. Each point will be predicted 100 times
control <- trainControl(method='repeatedcv', 
                        number=5, 
                        repeats=100,savePredictions ="all")

#define hyperparameter grid
n.trees=seq(1000,1600,200)
interaction.depth=2
shrinkage=c(.005,.007,.01,.05)
minobsinnode=c(3,5,7,10)
#make grid
tunegrid <- expand.grid(n.trees=n.trees,interaction.depth=interaction.depth,shrinkage=shrinkage,n.minobsinnode=minobsinnode)

#perform cv process
boost<- train(Y~.,data=allData, method='gbm',tuneGrid=tunegrid, trControl=control, verbose=F)

#visualize results
plot(boost)

#obtain all predictions made in cv process
allPreds<-boost$pred
#obtain best hyperparameter values
FinalNumTrees<-boost[["bestTune"]][1]
FinalShrinkage<-boost[["bestTune"]][3]
Finalminobs<-boost[["bestTune"]][4]
FinalInteractionDepth<-2 #let's choose depth of 2 just for interpretability (since 3 and 4 didn't improve things much. This is a fairly arbitrary choice, however, and can change)

#obtain individual predictions we want
finalSubset<-subset(allPreds,n.trees==as.numeric(FinalNumTrees) & shrinkage==as.numeric(FinalShrinkage) & interaction.depth==FinalInteractionDepth &n.minobsinnode==as.numeric(Finalminobs))

preds<-finalSubset$pred
actual<-finalSubset$obs

#get same boxplot of CV RMSE as in random forest section
allRMSEBoost <-c()
index<-1
for(i in 1:100){
  predsTemp<-preds[index:(index+nrow(allData)-1)]
  actualTemp<-actual[index:(index+nrow(allData)-1)]
  preds_untrsf <- predsTemp^3
  actual_untrsf <-  actualTemp^3
  allRMSEBoost <- c(allRMSEBoost,compute_rMSE(preds_untrsf, actual_untrsf))
  index<-index+nrow(allData)
}
boxplot(allRMSEBoost)

#get all predictions on original scale
preds_untrsf <- preds^3
actual_untrsf <-  actual^3

#investigate overall metrics
compute_rMSE(preds_untrsf, actual_untrsf)
cor(preds_untrsf, actual_untrsf, method="spearman")


#investigate individual countries

#get predictions where row index was
USpreds<-finalSubset[finalSubset$rowIndex == index.USA,1]
Canadapreds<-finalSubset[finalSubset$rowIndex == index.CAN,1]
Koreapreds <- finalSubset[finalSubset$rowIndex == index.KOR,1] 
Mexicopreds <- finalSubset[finalSubset$rowIndex == index.MEX,1] 
Germanypreds <- finalSubset[finalSubset$rowIndex == index.DEU,1] 
Myanmarpreds <- finalSubset[finalSubset$rowIndex == index.MMR,1] 

#get these predictions on original scale
us_preds_untrsf <-  USpreds^3
can_preds_untrsf <- Canadapreds^3
kor_preds_untrsf <- Koreapreds^3

deu_preds_untrsf <- Germanypreds^3
mex_preds_untrsf <- Mexicopreds^3
mmr_preds_untrsf <- Myanmarpreds^3
# us_preds_untrsf <-  USpreds
# can_preds_untrsf <- Canadapreds

#get actual US and Canada values
us_actual<-Y[index.USA]^3
canada_actual<-Y[index.CAN]^3
korea_actual<-Y[index.KOR]^3
germany_actual<-Y[index.DEU]^3
mexico_actual<-Y[index.MEX]^3
myanmar_actual<-Y[index.MMR]^3


#investigate
paste0("USA observed: ",us_actual)
paste0("USA predicted: ", mean(us_preds_untrsf))

paste0("CAN observed: ",canada_actual)
paste0("CAN predicted: ", mean(can_preds_untrsf))

paste0("KOR observed: ", korea_actual)
paste0("KOR predicted: ", mean(kor_preds_untrsf))

paste0("DEU observed: ", germany_actual)
paste0("DEU predicted: ", mean(deu_preds_untrsf))

paste0("MEX observed: ", mexico_actual)
paste0("MEX predicted: ", mean(mex_preds_untrsf))

paste0("MMR observed: ", myanmar_actual)
paste0("MMR predicted: ", mean(mmr_preds_untrsf))

# Make same variable importance plot as in paper
# use importance found throughout process
varImportance<-varImp(boost)
varImportance<- as.data.frame(varImportance$importance)
varImportance$varnames <- rownames(varImportance) # row names to column
rownames(varImportance) <- NULL  
varImportance<- varImportance[order(-varImportance$Overall),]
#varImportance<- varImportance[1:20,] #c(1,3,9,12,15)
#varImportance$var_categ <- c(2,2,2,2,2) # random var category
#plot
ggplot(varImportance, aes(x=reorder(varnames, Overall), y=Overall,color=as.factor(varImportance$var_categ))) + 
  geom_point(lwd=3, color="#00BFC4") +
  geom_segment(color="#00BFC4",aes(x=varnames,xend=varnames,y=0,yend=Overall)) +
  scale_color_discrete(name="Feature Type", labels = c("Modifiable")) +
  ylab("IncNodePurity") +
  xlab("Variable Name") +
  labs(x ="", y = "Relative Importance")+
  coord_flip()+theme(axis.text=element_text(size=15),
        axis.title=element_text(size=15),legend.text=element_text(size=15),legend.title=element_text(size=15))+
  theme(aspect.ratio=.9/1)+
scale_shape_manual(name = "A", values=c(Y_A = 0, Y_A_obs = 15), 
                     labels=c(Y_A = "y_true", Y_A_obs = "y_pred"), 
                     guide = guide_legend(order = 1))
```

#### Partial Dependency Plots as in Paper
```{r}
par(mfrow=c(2,2))

##############
#   1-d PDP  #    
##############
VaxPartial<- partial(boost, pred.var = "Percent_One_Dose_As_Of_Nov_1", train=allData[,-20])
plot(VaxPartial,main="Percent_One_Dose_As_Of_Nov_1",type="l",lwd=4,col="cyan3",xlab="% with >= 1 Dose",cex.axis=1.5,cex.lab=1.5,cex.main=1.5)

ObesePartial<- partial(boost, pred.var = "Trust_Covid_Advice_Govt", train=allData[,-20])
plot(ObesePartial,type="line",main="Trust_Covid_Advice_Govt",lwd=4,col="cyan3",cex.axis=1.5,xlab="% w/ BMI >= 30",cex.lab=1.5,cex.main=1.5)

ObesePartial<- partial(boost, pred.var = "Trust_In_Govt", train=allData[,-20])
plot(ObesePartial,type="line",main="Trust_In_Govt",lwd=4,col="cyan3",cex.axis=1.5,xlab="% w/ BMI >= 30",cex.lab=1.5,cex.main=1.5)

ObesePartial<- partial(boost, pred.var = "Confidence_In_Hospitals", train=allData[,-20])
plot(ObesePartial,type="line",main="Confidence_In_Hospitals",lwd=4,col="cyan3",cex.axis=1.5,xlab="% w/ BMI >= 30",cex.lab=1.5,cex.main=1.5)

NeighborhoodPartial<- partial(boost, pred.var = "Govt_Swiftness_Stringency", train=allData[,-20])
plot(NeighborhoodPartial,main="Govt_Swiftness_Stringency",type="l",lwd=4,col="cyan3",cex.axis=1.5,xlab="% Trusting Others in Neighborhood",cex.lab=1.5,cex.main=1.5)

NeighborhoodPartial<- partial(boost, pred.var = "Govt_Persistent_Stringency", train=allData[,-20])
plot(NeighborhoodPartial,main="Govt_Persistent_Stringency",type="l",lwd=4,col="cyan3",cex.axis=1.5,xlab="% Trusting Others in Neighborhood",cex.lab=1.5,cex.main=1.5)

NeighborhoodPartial<- partial(boost, pred.var = "Avg_Test_", train=allData[,-20])
plot(NeighborhoodPartial,main="Govt_Persistent_Stringency",type="l",lwd=4,col="cyan3",cex.axis=1.5,xlab="% Trusting Others in Neighborhood",cex.lab=1.5,cex.main=1.5)


##############
#   3-d PDP  #    
##############

predVar="Percent_One_Dose_As_Of_Nov_1"
predVar2="Obese_Adult_Percentage"

#predVar="GDP_Per_Capita"
#predVar2="Trust_Covid_Advice_Govt"

a<-boost %>%
partial(pred.var = c(predVar, predVar2), chull = TRUE, progress = TRUE)
dens <- akima::interp(x = a[,1], y = a[,2], z = a$yhat)
dens$z<-dens$z^3
a$yhat<-a$yhat^3
p3 <- plot_ly(x = dens$y, 
          y = dens$x, 
          z = dens$z,
          colors = c("blue", "grey", "red"),
          type = "surface")
p3 <- p3 %>% layout(scene = list(xaxis = list(title = "% Trust Govt COVID Advice"),
                             yaxis = list(title = "GDP Per Capita"),
                             zaxis = list(title = "Partial Dependence")))
show(p3)
```

